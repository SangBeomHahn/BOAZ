{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1차 시도를 극복해보자\n",
        "\n",
        "### -> 총 3가지 도전을 해볼 것이다.\n",
        "\n",
        "1) resnet으로 예측\n",
        "\n",
        "2) 내 모델을 224 by 224 by 3에 맞게 만들어져 있으니깐 mnist를 224 by 224로 만들어서 학습시키고 예측해보자(\"해상도를 높였으니 잘 알아보지 않을까??\"라는 가능성의 실험)\n",
        "\n",
        "3) 그레이 스케일을 잘 해보자(유튜브 참고)"
      ],
      "metadata": {
        "id": "tV_FFZ17QEsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) resnet으로 예측"
      ],
      "metadata": {
        "id": "ghASADRJQ6fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw43wB4HHXT6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# include_top을 하면 vgg 분류기까지 가져온다는 얘기\n",
        "model = VGG16(weights='imagenet', include_top=True) \n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
        "\n",
        "# resize into VGG16 trained images' format\n",
        "im = cv2.resize(cv2.imread('/content/123.jpg'), (224, 224))\n",
        "im = np.expand_dims(im, axis=0) # 1, 224, 224, 3이 됨\n",
        "# im = im.reshape(1,im.shape[0],im.shape[1], im.shape[2])\n",
        "im.astype(np.float32)\n",
        "\n",
        "print(type(im))\n",
        "\n",
        "# predict\n",
        "out = model.predict(im)\n",
        "index = np.argmax(out)\n",
        "print(index)\n",
        "\n",
        "#plt.plot(out.ravel())\n",
        "#plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3oOyADCRBUT",
        "outputId": "d5fc2e0d-4283-4fdd-bbad-d114d9682508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f47bae27290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 넘파이화 실험\n",
        "\n",
        "-> 입력 이미지를 반드시 넘파이화 해야 모델 학습을 할 수 있다."
      ],
      "metadata": {
        "id": "1mB3krIhR2NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# include_top을 하면 vgg 분류기까지 가져온다는 얘기\n",
        "model = VGG16(weights='imagenet', include_top=True) \n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
        "\n",
        "# resize into VGG16 trained images' format\n",
        "im = cv2.resize(cv2.imread('/content/123.jpg'), (224, 224))\n",
        "im = np.array(im)\n",
        "im = np.expand_dims(im, axis=0) # 1, 224, 224, 3이 됨\n",
        "im.astype(np.float32)\n",
        "\n",
        "print(type(im))\n",
        "\n",
        "# predict\n",
        "out = model.predict(im)\n",
        "index = np.argmax(out)\n",
        "print(index)\n",
        "\n",
        "#plt.plot(out.ravel())\n",
        "#plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYEY14IJRYZt",
        "outputId": "3739ca91-5582-4c1c-96e3-8c36e9c8b717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iiQ-Gb2SuoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHmTaO7XTW8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPiaCOEyTW52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 내 모델을 224 by 224 by 3에 맞게 만들고 mnist를 224 by 224로 만들어서 학습시키고 높은 해상도로 예측해보자\n",
        "\n",
        "궁금증 : 어떠한 경우에 모델 input이 32, 32이고 어떨떈 32, 32, 3이지? -> 채널이 1이 아니면 무조건 3을 붙어야 하나?\n",
        "<br><br>\n",
        "\n",
        "해결 : 3차원 컬러 이미지이면 모델 input shape를 32 * 32 * 3으로하고 학습 이미지도 60000 * 32 * 32 * 3으로 reshape해야하고 inference로 넣을 이미지도 1 * 32 * 32 * 3으로 넣어야한다.\n",
        "\n",
        "-> 하지만 1차원 이미지이면 컬러 차원을 넣을 필요가 없다.\n"
      ],
      "metadata": {
        "id": "_Xj7nCtFTXM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "import cv2\n",
        "\n",
        "#define the convnet \n",
        "class LeNet:\n",
        "\t@staticmethod\n",
        "\tdef build(input_shape, classes):\n",
        "\t\tmodel = models.Sequential()\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tmodel.add(layers.Convolution2D(20, (5, 5), activation='relu',\n",
        "\t\t\tinput_shape=INPUT_SHAPE))\n",
        "\t\tmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tmodel.add(layers.Convolution2D(50, (5, 5), activation='relu'))\n",
        "\t\tmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\t\t# Flatten => RELU layers\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\tmodel.add(layers.Dense(500, activation='relu'))\n",
        "\t\t# a softmax classifier\n",
        "\t\tmodel.add(layers.Dense(classes, activation=\"softmax\"))\n",
        "\t\treturn model\n",
        "\n",
        "\n",
        "\n",
        "# network and training\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "VALIDATION_SPLIT=0.90\n",
        "\n",
        "IMG_ROWS, IMG_COLS = 224, 224 # input image dimensions\n",
        "INPUT_SHAPE = (IMG_ROWS, IMG_COLS)\n",
        "NB_CLASSES = 10  # number of outputs = number of digits\n",
        "\n",
        "# data: shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# reshape\n",
        "X_train = X_train.reshape((60000, 28, 28, 1))\n",
        "X_test = X_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8t58cu7TYBq",
        "outputId": "4f3a3f60-c026-4a67-a87f-e4382a110a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cv로 하면 한장 한장씩 resize 해야하는데? tf로하면 한 번에 된다. 하지만 mnist의 경우 60000만 장으로 동작 도중 연결이 끊긴다.\n",
        "\n",
        "-> resize랑 reshape만 잘쓰면 되네~ 근데 mnist 784개가 224 * 224만큼 안 나와서 안된다."
      ],
      "metadata": {
        "id": "cYmCH8NWVnnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# im = cv2.resize(cv2.imread('/content/123.jpg'), (224, 224))\n",
        "# X_train = cv2.resize(X_train, (224, 224))\n",
        "# X_test = cv2.resize(X_test, (224, 224))"
      ],
      "metadata": {
        "id": "2eCDkaSYU5hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = tf.image.resize(X_train, [224, 224])\n",
        "# X_test = tf.image.resize(X_test, [224, 224])\n",
        "# print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "BjFlH2KzVTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.reshape((-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "VjovYGwUrEQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(X_train.shape[0]):\n",
        "  X_train[i] = X_train[i].reshape((-1, 224, 224, 1))\n",
        "\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "u83hAf51eUXh",
        "outputId": "96d00878-7c47-4b87-bb53-7c56902632ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c8207e1a7b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 784 into shape (224,224,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# cast\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n",
        "\n",
        "# initialize the optimizer and model\n",
        "model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# use TensorBoard, princess Aurora!\n",
        "callbacks = [\n",
        "  # Write TensorBoard logs to `./logs` directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "\n",
        "# fit \n",
        "history = model.fit(X_train, y_train, \n",
        "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS, \n",
        "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT,\n",
        "\t\tcallbacks=callbacks)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
        "print(\"\\nTest score:\", score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "s3qeGw_CTs0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9H-rvP7hFRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0LnH3kChFP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbJGNcSkhFMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uwAa8LUhFKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riAyKH6rhFID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sISPXELKhFGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 끊질긴 노력 끝에 성공~\n",
        "\n",
        "# 3) gray scale \n",
        "\n",
        "https://copycoding.tistory.com/225\n",
        "\n",
        "○ CV\n",
        "\n",
        "-> 이미지 입력 shape 전처리 단계 정리\n",
        "\n",
        "1.img = cv2.imread(\"/content/제목 없음.png\", cv2.2/IMREAD_GRAYSCALE)\n",
        "\n",
        "2.img = cv2.resize(255-img, (28, 28)) # 이것만 해도 원래 안됐던 reshape은 됨\n",
        "\n",
        "3.img = img.flatten() / 255.0\n",
        "\n",
        "4.img = img.reshape((-1, 28, 28, 1))\n",
        "<br><br>\n",
        "※부수적인 것 : 5. 실수화 : im.astype(np.float32) ↑ 이 위에서 넘파이화는 이미 되어있다/"
      ],
      "metadata": {
        "id": "C9vrK2QyhFkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "import cv2"
      ],
      "metadata": {
        "id": "-njIqtE-LlVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "9W7E-OWThItw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d552093-95c0-466e-cd29-974f05b58800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((60000, 28, 28, 1))\n",
        "X_test = X_test.reshape((10000, 28, 28, 1))"
      ],
      "metadata": {
        "id": "jGBGF5lCkQPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "metadata": {
        "id": "zfbcVGWghhPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "\n",
        "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
        "IMG_CHANNELS = 1\n",
        "IMG_ROWS = 28\n",
        "IMG_COLS = 28\n",
        "\n",
        "#constant\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "CLASSES = 10\n",
        "VERBOSE = 1\n",
        "VALIDATION_SPLIT = 0.2\n",
        "OPTIM = tf.keras.optimizers.RMSprop()\n",
        "\n",
        "#define the convnet \n",
        "def build(input_shape, classes):\n",
        "\tmodel = models.Sequential() \n",
        "\tmodel.add(layers.Convolution2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=input_shape))\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(layers.Dropout(0.25)) \n",
        "\n",
        "\tmodel.add(layers.Flatten())\n",
        "\tmodel.add(layers.Dense(512, activation='relu'))\n",
        "\tmodel.add(layers.Dropout(0.5))\n",
        "\tmodel.add(layers.Dense(classes, activation='softmax'))\n",
        "\treturn model\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, CLASSES)\n",
        "\n",
        "model=build((IMG_ROWS, IMG_COLS, IMG_CHANNELS), CLASSES)\n",
        "# model=build((IMG_ROWS, IMG_COLS), CLASSES)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
        "\tmetrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDH_tkJjh0pA",
        "outputId": "bdf701da-b56c-421e-b4d7-60639a288f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 13, 13, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2769408   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,774,858\n",
            "Trainable params: 2,774,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "CQLLbmXjiPez",
        "outputId": "ed21eda1-4dde-4213-90ef-5683be79e243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 451/1875 [======>.......................] - ETA: 1:44 - loss: 0.3646 - accuracy: 0.8872"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-00d921b4d0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
        "print(\"\\nTest score:\", score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "IbGLAVImiYEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread('/content/다운로드.png')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "FqH1YsX-m1Tm",
        "outputId": "f08db27f-0fd7-43a7-8440-af01f701c2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f351df33fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUF0lEQVR4nO3df4zU9Z3H8edrd2ENavEXIIWlrAah1uSQEiXa2p5eK9K7oib1MJdqPVO00VQTLx7aPzSXNqmetknrqdGIovG03mHVJnjWM2219UQXpfgDLeAvwGVBqwsssMvuvu+P+a4dcSnLfmf4zs7n9Ugm853P9zsz72V2X3y/35n5vBURmFm6GoouwMyK5RAwS5xDwCxxDgGzxDkEzBLnEDBLXNVCQNJcSW9IWitpUbWex8zyUTU+JyCpEfgT8DVgA/ACcH5EvFbxJzOzXKq1J3ASsDYi3oyIHuBBYH6VnsvMcmiq0uNOAtaX3d4AnLy3jY866qiYOnVqlUoxM4AVK1a8HxHj9hyvVgjsk6SFwEKAKVOm0NbWVlQpZkmQ9M5g49U6HNgItJTdnpyNfSwi7oiI2RExe9y4T4WTmR0g1QqBF4BpkloljQYWAI9V6bnMLIeqHA5ERK+ky4EngEZgcUS8Wo3nMrN8qnZOICKWAcuq9fhmVhn+xKBZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglbtghIKlF0m8kvSbpVUlXZOPXS9ooaWV2mVe5cs2s0vJMKtILXBURL0o6FFgh6cls3U8j4qb85ZlZtQ07BCKiHWjPlrdJWk1pqnEzG0Eqck5A0lTgRGB5NnS5pFWSFks6vBLPYWbVkTsEJB0CLAWujIitwG3AscBMSnsKN+/lfgsltUlq27JlS94yzGyYcoWApFGUAuD+iHgYICI6IqIvIvqBOym1JPsU9x0wqw153h0QcBewOiJ+UjY+sWyzc4BXhl+emVVbnncHTgW+DbwsaWU2di1wvqSZQABvA5fkqtDMqirPuwO/BzTIKvcaMBtB/IlBs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxeWYWAkDS28A2oA/ojYjZko4AfgFMpTS70HkR8WHe5zKzyqvUnsDfRsTMiJid3V4EPBUR04CnsttmVoOqdTgwH1iSLS8Bzq7S85hZTpUIgQB+LWmFpIXZ2ISsQxHAJmDCnndy3wGz2pD7nADwpYjYKGk88KSk18tXRkRIij3vFBF3AHcAzJ49+1PrzezAyL0nEBEbs+vNwC8pNRvpGOg/kF1vzvs8ZlYdeTsQHZx1JEbSwcDXKTUbeQy4MNvsQuDRPM9jZtWT93BgAvDLUjMimoD/jIj/kfQC8JCki4F3gPNyPo+ZVUmuEIiIN4G/GWT8A+CMPI9tZgeGPzFoljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJq8R3B6wGRNT+1y+yD5VZjXEI1Im33nqL119/ne7u7pr5Yxs1ahRjxoyhpaWFz33uczQ0NCDp44vVBodAnWhra+Paa69l3bp1RZeyV/PmzWPu3LmceeaZtLa20tjYSEODj0iL5legTjQ1NdX8H9SyZcv4/ve/z/Tp01mwYAHLly+nu7t7RBzK1LPa/q2xuvXwww9z6qmn8t3vfpd3332X3bt3F11SshwCVpiI4L777uP444/nV7/6FTt37iy6pCQ5BKxwO3bs4IILLuDuu++mq6ur6HKS4xCwmtDV1cU111zDPffcw44dO4ouJynDDgFJ0yWtLLtslXSlpOslbSwbn1fJgq1+bd26leuvv56lS5f6HMEBNOwQiIg3sl4DM4EvAjsozTEI8NOBdRGxrBKFWhref/99rrvuOl566aWiS0lGpQ4HzgDWRcQ7FXo8S1h7ezs333wznZ2dRZeShEqFwALggbLbl0taJWmxpMMHu4P7Dtje7Nq1i+eee44nnnii6FKSkDsEJI0Gvgn8VzZ0G3AsMBNoB24e7H4RcUdEzI6I2ePGjctbhtWZTZs28cADD/DRRx8VXUrdq8SewFnAixHRARARHRHRFxH9wJ2U+hCY7Zeenh5ef/112traii6l7lUiBM6n7FBgoOlI5hxKfQjM9tvmzZt5+umn6e3tLbqUupbrC0RZw5GvAZeUDd8oaSalHoVv77HObMg6OztZtWoV27dv57DDDiu6nLqVt+9AF3DkHmPfzlWRWaavr4/NmzfT0dHhEKgif2LQalpXVxfvvfde0WXUNYdAnejr66vLr+T29PSwbdu2osuoa55UpE58/vOf54orrqiZt9S6urq45ZZb2L59e67HiQj6+/srVJUNxiFQJ6ZNm8bkyZNr5g+mo6ODxYsX5w4Bqz6HQJ1obm6mubm56DI+tnPnTs8jOEL4nIBZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJG1IIZBOGbpb0StnYEZKelLQmuz48G5ekn0lam002OqtaxZtZfkPdE7gHmLvH2CLgqYiYBjyV3YbSnIPTsstCShOPmlmNGlIIRMTTwJ/3GJ4PLMmWlwBnl43fGyXPAYftMe+gmdWQPOcEJkREe7a8CZiQLU8C1pdttyEb+wT3HTCrDRU5MRilKW32a1ob9x2oTRFRsYuNDHnmE+iQNDEi2rPd/c3Z+EagpWy7ydmY1ZCBP9Te3l527dpFb28vfX199Pf3557iu7GxkY6OjpqZ4MT+ujwh8BhwIfDj7PrRsvHLJT0InAx0lh02WA3o7u6ms7OT9vZ23nzzTZ5//nnWr1/P+++/T2dnJ93d3bmfY/fu3Xz44Ye5H0eSJyepsiGFgKQHgK8CR0naAFxH6Y//IUkXA+8A52WbLwPmAWspdSq+qMI12zD19PTw7rvvsnLlSpYuXcrjjz9e800/Gxsba2rGpHo0pBCIiPP3suqMQbYN4LI8RVnlvffeezz//PP88Ic/ZMWKFUWXM2SjR4/m0EMPLbqMuuY5Butcb28vr732Grfddht33nknfX19RZe0Xw455BBaWlr2vaENm0Ogju3evZvf/va3XHfddSxfvnzEnagbNWoUEydOZPz48UWXUtccAnWqt7eXJ554gquuuoo1a9aMyLfsjjzySE455RQOOuigokupa/4CUZ169tlnWbRoEWvXrh2RAQAwfvx4Tj311KLLqHsOgTq0bt06brjhBtasWTPiDgEGjBkzhlmzZjF9+vSiS6l7DoE609PTw6233sozzzxDT09P0eUM22c/+1kuuugivzNwADgE6swf/vAHHnnkkRHdxHPs2LGce+65zJkzp+hSkuAQqCPbtm1j8eLFtLeP3A9oNjc385WvfIVLL72U0aNHF11OEhwCdWT16tWsXLmSnTt3Fl3KsDQ0NDB9+nSuvvpqWltbiy4nGQ6BOvLMM88wUr+WLYljjz2WG2+8kZNPPrnocpLiEKgTu3bt4tlnn+WDDz4oupT91tDQwCmnnMLtt9/OGWecQVOTP75yIPlfu05s2rSJTZs25f4a8IHW1NTEJZdcwve+9z1mzJhBY2Nj0SUlxyFQJ9avX8/27duLLmO/zJkzh0WLFnHyySdz9NFHF11OshwCdWLr1q0j4nMBY8eO5Rvf+Abf+ta3OPHEE5k4caLfBSiYQ6BOdHd3V+QbgqNGjWLGjBmMGjUq92M1NzczduxYxo0bR0tLC3PmzKG1tZWjjz6az3zmM/7jrxH7DAFJi4G/BzZHxAnZ2L8D/wD0AOuAiyLiI0lTgdXAG9ndn4uIS6tQt1XJEUccwZIlS5g0aVLuUGlqaqKhoYHGxsaPJwdpamrybEE1Zih7AvcAtwD3lo09CVwTEb2SbgCuAf41W7cuImZWtEo7YBobGxk/fjyVmvzVf+y1b59vEQ7WcyAifh0RA6ehn6M0majVkYH/rfNerPZV4nMC/ww8Xna7VdJLkn4n6csVeHwzq6JcJwYl/QDoBe7PhtqBKRHxgaQvAo9I+kJEbB3kvgsptSljypQpecowsxyGvScg6TuUThj+Uza5KBHRHREfZMsrKJ00PG6w+7v5iFltGFYISJoLXA18MyJ2lI2Pk9SYLR9DqSnpm5Uo1MyqYyhvEQ7Wc+AaoBl4Mjv5M/BW4GnAv0naDfQDl0bEno1MzayG7DME9tJz4K69bLsUWJq3KDM7cPwtQrPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxnlnIPqG/v5/Ozs6a6QTc0NDAmDFjaG5uLrqUuuUQsE/o6uri7rvv5uCDDy66FKA009Hpp5/OCSecUHQpdcshYJ+wbds2brrppqLL+Nhxxx3H5MmTHQJV5HMCVtMk0dDgX9Nq8r+uWeIcAmaJcwiYJW6fISBpsaTNkl4pG7te0kZJK7PLvLJ110haK+kNSWdWq3Azq4yh7AncA8wdZPynETEzuywDkHQ8sAD4QnafWwemGzOz2jSsvgN/xXzgwWzC0beAtcBJOeozsyrLc07gckmrssOFw7OxScD6sm02ZGNmVqOGGwK3AccCMyn1Grh5fx9A0kJJbZLatmzZMswyzCyvYYVARHRERF9E9AN38pdd/o1AS9mmk7OxwR7DfQfMasBw+w5MLLt5DjDwzsFjwAJJzZJaKfUdeD5fiWZWTcPtO/BVSTOBAN4GLgGIiFclPQS8Rqk92WURka+/tZlVVUX7DmTb/wj4UZ6izOzA8ScGzRLnEDBLnEOgTvT29tLf3190GRXX399Pb29v0WXUNU8qUidmz57Nz3/+c7q7u8maxI54EcFBBx3EjBkzii6lrjkE6kRrayutra1Fl2EjkEOgTtTL//524PmcgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4obbd+AXZT0H3pa0MhufKmln2brbq1m8meU3lI8N3wPcAtw7MBAR/ziwLOlmoLNs+3URMbNSBZpZdQ1lZqGnJU0dbJ1KH1g/Dzi9smWZ2YGS95zAl4GOiFhTNtYq6SVJv5P05ZyPb2ZVlvdbhOcDD5TdbgemRMQHkr4IPCLpCxGxdc87SloILASYMmVKzjLMbLiGvScgqQk4F/jFwFjWfuyDbHkFsA44brD7u++AWW3Iczjwd8DrEbFhYEDSuIEGpJKOodR34M18JZpZNQ3lLcIHgP8DpkvaIOnibNUCPnkoAHAasCp7y/C/gUsjYqjNTM2sAMPtO0BEfGeQsaXA0vxlmdmB4k8MmiXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeKGMqlIi6TfSHpN0quSrsjGj5D0pKQ12fXh2bgk/UzSWkmrJM2q9g9hZsM3lD2BXuCqiDgemANcJul4YBHwVERMA57KbgOcRWlasWmUJhK9reJVm1nF7DMEIqI9Il7MlrcBq4FJwHxgSbbZEuDsbHk+cG+UPAccJmlixSs3s4rYr3MCWROSE4HlwISIaM9WbQImZMuTgPVld9uQjZlZDRpyCEg6hNL8gVfu2UcgIgKI/XliSQsltUlq27Jly/7c1cwqaEghIGkUpQC4PyIezoY7Bnbzs+vN2fhGoKXs7pOzsU9w3wGz2jCUdwcE3AWsjoiflK16DLgwW74QeLRs/ILsXYI5QGfZYYOZ1ZihtCE7Ffg28PJAC3LgWuDHwENZH4J3KDUmBVgGzAPWAjuAiypasZlV1FD6Dvwe0F5WnzHI9gFclrMuMztA/IlBs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEpTAhZchLQF6ALeL7qWHI7C9RdtpP8M1a7/cxHxqfn9ayIEACS1RcTsousYLtdfvJH+MxRVvw8HzBLnEDBLXC2FwB1FF5CT6y/eSP8ZCqm/Zs4JmFkxamlPwMwKUHgISJor6Q1JayUtKrqeoZL0tqSXJa2U1JaNHSHpSUlrsuvDi65zgKTFkjZLeqVsbNB6s2ayP8tek1WSZhVX+ce1Dlb/9ZI2Zq/BSknzytZdk9X/hqQzi6n6LyS1SPqNpNckvSrpimy8+NcgIgq7AI3AOuAYYDTwR+D4Imvaj9rfBo7aY+xGYFG2vAi4oeg6y2o7DZgFvLKveik1lH2cUg/KOcDyGq3/euBfBtn2+Ox3qRlozX7HGguufyIwK1s+FPhTVmfhr0HRewInAWsj4s2I6AEeBOYXXFMe84El2fIS4OwCa/mEiHga+PMew3urdz5wb5Q8BxwmaeKBqXRwe6l/b+YDD0ZEd0S8RalD9klVK24IIqI9Il7MlrcBq4FJ1MBrUHQITALWl93ekI2NBAH8WtIKSQuzsQkR0Z4tbwImFFPakO2t3pH0ulye7S4vLjv8qun6JU0FTgSWUwOvQdEhMJJ9KSJmAWcBl0k6rXxllPbpRsxbLyOt3sxtwLHATKAduLnYcvZN0iHAUuDKiNhavq6o16DoENgItJTdnpyN1byI2JhdbwZ+SWl3s2Ngly273lxchUOyt3pHxOsSER0R0RcR/cCd/GWXvybrlzSKUgDcHxEPZ8OFvwZFh8ALwDRJrZJGAwuAxwquaZ8kHSzp0IFl4OvAK5RqvzDb7ELg0WIqHLK91fsYcEF2hnoO0Fm2y1oz9jhGPofSawCl+hdIapbUCkwDnj/Q9ZWTJOAuYHVE/KRsVfGvQZFnTMvOgv6J0hncHxRdzxBrPobS2ec/Aq8O1A0cCTwFrAH+Fzii6FrLan6A0i7zbkrHlxfvrV5KZ6T/I3tNXgZm12j992X1raL0RzOxbPsfZPW/AZxVA/V/idKu/ipgZXaZVwuvgT8xaJa4og8HzKxgDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEvc/wPM+22LYBNCXQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dc8smNrLxm1",
        "outputId": "17e4aa98-77d5-43be-ad6b-32a7d2c8a1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(225, 225, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.resize(img, (28, 28))"
      ],
      "metadata": {
        "id": "VuPtZT1SNZej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "t8jmb7h_Nq_k",
        "outputId": "c736d827-fb38-47f9-a722-c9085d24bbda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "img = cv2.imread(\"/content/제목 없음.png\", cv2.IMREAD_GRAYSCALE)"
      ],
      "metadata": {
        "id": "3wOzPm61nhOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6QKTaIkoZhh",
        "outputId": "df88742c-ff68-4752-ed25-5f01e635fc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(296, 323)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.resize(255-img, (28, 28)) # 이것만 해도 원래 안됐던 reshape은 됨"
      ],
      "metadata": {
        "id": "oXYKCu7aoiY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.flatten() / 255.0\n",
        "img = img.reshape((-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "Xd8FreQApAm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVnFLiJUpXKn",
        "outputId": "5c6fdb90-1f06-4b84-c951-8eccd97519de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.reshape((3, 28, 28))"
      ],
      "metadata": {
        "id": "MYylmpWdMx9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo92mljsM1A6",
        "outputId": "536cd8e0-a877-458e-ee2d-932449eb9570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDVJOfHGM06q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "xWrxm6IzowFU",
        "outputId": "a6250177-3df6-4657-f038-688e3b4219b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.permute(1, 2, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "oOfuNAkPM-_p",
        "outputId": "54d49c05-1ccb-4364-92b8-db7de748a85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9e50658deb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'permute'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model.predict(img, verbose=0) \n",
        "predicted = y_prob.argmax(axis=-1)"
      ],
      "metadata": {
        "id": "qUrMoz8Mn46d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 ~ 9 중 3번째인 2를 3이라고 잘 예측한다!!\n",
        "\n",
        "-> 이 프로젝트는 지금까지 모델을 훈련하고 검증만 해봤지 실제 사용해 본 경험이 없어 도전한 것이었고 이미지 처리에서 배운게 많은 경험이었다."
      ],
      "metadata": {
        "id": "AnZjv6LRLQEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xweXb1aepeMX",
        "outputId": "04e83559-f60b-4588-a489-b31eb7fe6bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIH-uU9mpoEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}